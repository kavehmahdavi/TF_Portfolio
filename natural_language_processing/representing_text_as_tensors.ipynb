{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Author: Kaveh Mahdavi <kavehmahdavi74@yahoo.com>\n",
    "License: BSD 3 clause\n",
    "last update: 28/12/2022"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Representing text as Tensors\n",
    "\n",
    "I explore different neural network architectures for dealing with natural language text by using:\n",
    "* bag-of-words\n",
    "* embeddings\n",
    "* recurrent neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# To use GPU memory cautiously, I set tensorflow option to grow GPU memory allocation when needed.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Represent text\n",
    "\n",
    "To solve Natural Language Processing (NLP) tasks with ANN, I need some way to represent text as tensors.\n",
    "\n",
    "* **Character-level representation:** I represent text by treating each character as a number. Given that we have C  different characters in our text corpus, the word Hello could be represented by a tensor with shape CÃ—5. Each letter would correspond to a tensor in one-hot encoding.\n",
    "*\n",
    "* **Word-level representation:** I create a vocabulary of all words in our text, and then represent words using one-hot encoding. This approach is better than character-level representation because each letter by itself does not have much meaning. By using higher-level semantic concepts - words - we simplify the task for the neural network. However, given a large dictionary size, we need to deal with high-dimensional sparse tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataset = tfds.load('ag_news_subset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 120000\n",
      "Size of test dataset:  7600\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(\"Size of train dataset: {}\".format(len(ds_train)))\n",
    "print(\"Size of test dataset:  {}\".format(len(ds_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(3), ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approaches to Represent Text as Tensor\n",
    "\n",
    "### 1. Bag-of-Words as Data Preprocessing\n",
    "\n",
    "I vectorize text into numbers to represent as tensors. In the word-level, I should do:\n",
    "* Use a tokenizer to split text into tokens.\n",
    "* Build a vocabulary of those tokens.\n",
    "\n",
    "I don't take to account words that are rarely present in the text, since only a few sentences will have them, and the model will not learn from them.\n",
    "I limit the vocabulary size by passing an argument to the `TextVectorization` constructor.\n",
    "\n",
    "#### 1.1. Vectorize & Build a Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for', '39s', 'with', 'that', 'its', 'as']\n",
      "Number of vocabulary: 5335\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 112, 3695, 5071, 3908])>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary[:15])\n",
    "print(f\"Number of vocabulary: {vocabulary_size}\")\n",
    "vectorizer('I love artificial intelligence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2. Bagging\n",
    "\n",
    "I convert each word number into a one-hot encoding and adding all those vectors up."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def get_bag_of_words(text, vocab_size):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text), vocab_size), axis=0)\n",
    "\n",
    "batch_size = 128\n",
    "ds_train_bow = ds_train.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                       x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                     x['label'])).batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3. Build Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 68s 72ms/step - loss: 0.6164 - acc: 0.8423 - val_loss: 0.4418 - val_acc: 0.8688\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocabulary_size,))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow, validation_data=ds_test_bow)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Bag-of-Words with n-grams\n",
    "\n",
    "Since some words are part of multi-word expressions, for example, the word 'on-line' has a completely different meaning.\n",
    "\n",
    "from the words 'on' and 'line' in other contexts. so the representation of 'on' and 'line' by the same vectors, it can confuse our model.\n",
    "\n",
    "IN n-gram the frequency of each word, bi-word or tri-word is a useful feature for training classifiers, e.g. bigram\n",
    "adds all word pairs to the vocabulary, in addition to original words.\n",
    "\n",
    "#### 2.1 Generate a bi-gram Bag-of-Words\n",
    "\n",
    "To use an n-gram representation in our AG News dataset, we need to pass the ngrams parameter to our TextVectorization constructor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kaveh/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kaveh/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for', '39s', 'with', 'that', 'its', 'as']\n",
      "Number of vocabulary: 20274\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([  130, 11718, 18382, 12901,     1,     1, 18381])>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000,ngrams=2)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary[:15])\n",
    "print(f\"Number of vocabulary: {vocabulary_size}\")\n",
    "vectorizer('I love artificial intelligence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2. Bagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "ds_train_bow_ngram = ds_train.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                       x['label'])).batch(batch_size)\n",
    "ds_test_bow_ngram = ds_test.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                     x['label'])).batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3. Build Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 294s 312ms/step - loss: 0.5812 - acc: 0.8499 - val_loss: 0.4235 - val_acc: 0.8708\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, 4)                 81100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,100\n",
      "Trainable params: 81,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocabulary_size,))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow_ngram, validation_data=ds_test_bow_ngram)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Bag-of-Words as a Layer\n",
    "\n",
    "Since the vectorizer is also a Keras layer, I can define a network that includes it, and train it end-to-end.\n",
    "\n",
    "Then I don't need to vectorize the dataset using map, we can just pass the original dataset to the input of the network.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Vectorize Text\n",
    "vectorizer_layer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000)\n",
    "vectorizer_layer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer_layer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 117s 124ms/step - loss: 0.6080 - acc: 0.8387 - val_loss: 0.4205 - val_acc: 0.8751\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_7 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " tf.one_hot_2 (TFOpLambda)   (None, None, 5335)        0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_2 (TFOpL  (None, 5335)             0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def to_tuple(_x):\n",
    "    return _x['title'] + ' ' + _x['description'], _x['label']\n",
    "\n",
    "batch_size = 128\n",
    "ds_train_embed = ds_train.map(to_tuple).batch(batch_size)\n",
    "ds_test_embed = ds_test.map(to_tuple).batch(batch_size)\n",
    "\n",
    "inp = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer_layer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x, vocabulary_size), axis=1)\n",
    "out = keras.layers.Dense(4, activation='softmax')(x)\n",
    "model = keras.models.Model(inp, out)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_embed, validation_data=ds_test_embed)\n",
    "\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Auto-compute Bag-of-Words\n",
    "\n",
    "Until here, I compute the BoW vectors manually by summing the one-hot encodings of individual words to show the calculation approach.\n",
    "\n",
    "To define and train the model easier, TensorFlow enable us to calculate BoW vectors automatically by passing the  `output_mode='count parameter'` to the vectorizer constructor.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer:\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.5920 - acc: 0.8489 - val_loss: 0.4166 - val_acc: 0.8772\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 5334)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 21340     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,340\n",
      "Trainable params: 21,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocabulary_size,output_mode='count'),\n",
    "    keras.layers.Dense(4,input_shape=(vocabulary_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer:\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train_embed,validation_data=ds_test_embed)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
