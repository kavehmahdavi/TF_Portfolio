{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Author: Kaveh Mahdavi <kavehmahdavi74@yahoo.com>\n",
    "License: BSD 3 clause\n",
    "last update: 28/12/2022"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Representing text as Tensors\n",
    "\n",
    "I explore different neural network architectures for dealing with natural language text by using:\n",
    "* bag-of-words\n",
    "* embeddings\n",
    "* recurrent neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# To use GPU memory cautiously, I set tensorflow option to grow GPU memory allocation when needed.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Represent text\n",
    "\n",
    "To solve Natural Language Processing (NLP) tasks with ANN, I need some way to represent text as tensors.\n",
    "\n",
    "* **Character-level representation:** I represent text by treating each character as a number. Given that we have C  different characters in our text corpus, the word Hello could be represented by a tensor with shape CÃ—5. Each letter would correspond to a tensor in one-hot encoding.\n",
    "*\n",
    "* **Word-level representation:** I create a vocabulary of all words in our text, and then represent words using one-hot encoding. This approach is better than character-level representation because each letter by itself does not have much meaning. By using higher-level semantic concepts - words - we simplify the task for the neural network. However, given a large dictionary size, we need to deal with high-dimensional sparse tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Dataset\n",
    "\n",
    "[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) dataset is a collection of more than 1  million news articles. News articles have been gathered from more than\n",
    "2000  news sources by ComeToMyHead in more than 1 year of activity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset = tfds.load('ag_news_subset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 120000\n",
      "Size of test dataset:  7600\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(\"Size of train dataset: {}\".format(len(ds_train)))\n",
    "print(\"Size of test dataset:  {}\".format(len(ds_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(3), ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approaches to Represent Text as Tensor\n",
    "\n",
    "### 1. Bag-of-Words as Data Preprocessing\n",
    "\n",
    "I vectorize text into numbers to represent as tensors. In the word-level, I should do:\n",
    "* Use a tokenizer to split text into tokens.\n",
    "* Build a vocabulary of those tokens.\n",
    "\n",
    "I don't take to account words that are rarely present in the text, since only a few sentences will have them, and the model will not learn from them.\n",
    "I limit the vocabulary size by passing an argument to the `TextVectorization` constructor.\n",
    "\n",
    "#### 1.1. Vectorize & Build a Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for', '39s', 'that', 'at', 'as', 'with']\n",
      "Number of vocabulary: 8317\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 178, 5930, 7893, 6226])>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size limiting\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000)\n",
    "vectorizer.adapt(ds_train.take(1000).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary[:15])\n",
    "print(f\"Number of vocabulary: {vocabulary_size}\")\n",
    "vectorizer('I love artificial intelligence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2. Bagging\n",
    "\n",
    "I convert each word number into a one-hot encoding and adding all those vectors up."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_bag_of_words(text, vocab_size):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text), vocab_size), axis=0)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "ds_train_bow = ds_train.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                       x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                     x['label'])).batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3. Build Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 68s 72ms/step - loss: 0.6164 - acc: 0.8423 - val_loss: 0.4418 - val_acc: 0.8688\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocabulary_size,))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow, validation_data=ds_test_bow)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Bag-of-Words with n-grams\n",
    "\n",
    "Since some words are part of multi-word expressions, for example, the word 'on-line' has a completely different meaning.\n",
    "\n",
    "from the words 'on' and 'line' in other contexts. so the representation of 'on' and 'line' by the same vectors, it can confuse our model.\n",
    "\n",
    "IN n-gram the frequency of each word, bi-word or tri-word is a useful feature for training classifiers, e.g. bigram\n",
    "adds all word pairs to the vocabulary, in addition to original words.\n",
    "\n",
    "#### 2.1 Generate a bi-gram Bag-of-Words\n",
    "\n",
    "To use an n-gram representation in our AG News dataset, we need to pass the ngrams parameter to our TextVectorization constructor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for', '39s', 'with', 'that', 'its', 'as']\n",
      "Number of vocabulary: 20274\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([  130, 11718, 18382, 12901,     1,     1, 18381])>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000, ngrams=2)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary[:15])\n",
    "print(f\"Number of vocabulary: {vocabulary_size}\")\n",
    "vectorizer('I love artificial intelligence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2. Bagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "ds_train_bow_ngram = ds_train.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                             x['label'])).batch(batch_size)\n",
    "ds_test_bow_ngram = ds_test.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                           x['label'])).batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3. Build Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 347s 370ms/step - loss: 0.5827 - acc: 0.8484 - val_loss: 0.4239 - val_acc: 0.8707\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 81100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,100\n",
      "Trainable params: 81,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocabulary_size,))\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow_ngram, validation_data=ds_test_bow_ngram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Bag-of-Words as a Layer\n",
    "\n",
    "Since the vectorizer is also a Keras layer, I can define a network that includes it, and train it end-to-end.\n",
    "\n",
    "Then I don't need to vectorize the dataset using map, we can just pass the original dataset to the input of the network.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Vectorize Text\n",
    "vectorizer_layer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000)\n",
    "vectorizer_layer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer_layer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def to_tuple(_x):\n",
    "    return _x['title'] + ' ' + _x['description'], _x['label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "ds_train_embed = ds_train.map(to_tuple).batch(batch_size)\n",
    "ds_test_embed = ds_test.map(to_tuple).batch(batch_size)\n",
    "\n",
    "inp = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer_layer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x, vocabulary_size), axis=1)\n",
    "out = keras.layers.Dense(4, activation='softmax')(x)\n",
    "model = keras.models.Model(inp, out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_embed, validation_data=ds_test_embed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Auto-compute Bag-of-Words\n",
    "\n",
    "Until here, I compute the BoW vectors manually by summing the one-hot encodings of individual words to show the calculation approach.\n",
    "\n",
    "To define and train the model easier, TensorFlow enable us to calculate BoW vectors automatically by passing the  `output_mode='count parameter'` to the vectorizer constructor.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer:\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.5920 - acc: 0.8489 - val_loss: 0.4166 - val_acc: 0.8772\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 5334)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 21340     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,340\n",
      "Trainable params: 21,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocabulary_size, output_mode='count'),\n",
    "    keras.layers.Dense(4, input_shape=(vocabulary_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer:\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_embed, validation_data=ds_test_embed)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Term frequency - Inverse Document Frequency\n",
    "\n",
    "It's a variation of bag-of-words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the frequency of the word occurrence in the corpus.\n",
    "\n",
    "The weight $w_{ij}$ of a word $i$ in the document $j$ is defined as:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "where\n",
    "* $tf_{ij}$ is the number of occurrences of $i$ in $j$, i.e. the BoW value we have seen before\n",
    "* $N$ is the number of documents in the collection\n",
    "* $df_i$ is the number of documents containing the word $i$ in the whole collection\n",
    "\n",
    "The TF-IDF value wij increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appears in every document in the collection, dfi=N, and wij=0, and those terms would be completely disregarded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 15s 15ms/step - loss: 0.4173 - acc: 0.8681 - val_loss: 0.3410 - val_acc: 0.8883\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_10 (Text  (None, 5334)             1         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 21340     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,341\n",
      "Trainable params: 21,340\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "ds_train_embed = ds_train.map(to_tuple).batch(batch_size)\n",
    "ds_test_embed = ds_test.map(to_tuple).batch(batch_size)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocabulary_size,output_mode='tf-idf'),\n",
    "    keras.layers.Dense(4,input_shape=(vocabulary_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train_embed,validation_data=ds_test_embed)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
