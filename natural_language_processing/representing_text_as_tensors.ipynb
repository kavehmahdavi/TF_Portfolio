{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Author: Kaveh Mahdavi <kavehmahdavi74@yahoo.com>\n",
    "License: BSD 3 clause\n",
    "last update: 28/12/2022"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Representing text as Tensors\n",
    "\n",
    "I explore different neural network architectures for dealing with natural language text by using:\n",
    "* bag-of-words\n",
    "* embeddings\n",
    "* recurrent neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# To use GPU memory cautiously, I set tensorflow option to grow GPU memory allocation when needed.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Represent text\n",
    "\n",
    "To solve Natural Language Processing (NLP) tasks with ANN, I need some way to represent text as tensors.\n",
    "\n",
    "* **Character-level representation:** I represent text by treating each character as a number. Given that we have C  different characters in our text corpus, the word Hello could be represented by a tensor with shape CÃ—5. Each letter would correspond to a tensor in one-hot encoding.\n",
    "*\n",
    "* **Word-level representation:** I create a vocabulary of all words in our text, and then represent words using one-hot encoding. This approach is better than character-level representation because each letter by itself does not have much meaning. By using higher-level semantic concepts - words - we simplify the task for the neural network. However, given a large dictionary size, we need to deal with high-dimensional sparse tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "dataset = tfds.load('ag_news_subset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 120000\n",
      "Size of test dataset:  7600\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(\"Size of train dataset: {}\".format(len(ds_train)))\n",
    "print(\"Size of test dataset:  {}\".format(len(ds_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(3), ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approaches to Represent Text as Tensor\n",
    "\n",
    "### 1. Bag-of-Words\n",
    "\n",
    "I vectorize text into numbers to represent as tensors. In the word-level, I should do:\n",
    "* Use a tokenizer to split text into tokens.\n",
    "* Build a vocabulary of those tokens.\n",
    "\n",
    "I don't take to account words that are rarely present in the text, since only a few sentences will have them, and the model will not learn from them.\n",
    "I limit the vocabulary size by passing an argument to the `TextVectorization` constructor.\n",
    "\n",
    "#### 1.1. Vectorize & Build a Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for', '39s', 'with', 'that', 'its', 'as']\n",
      "Number of vocabulary: 5335\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 112, 3695, 5071, 3908])>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))\n",
    "\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary[:15])\n",
    "print(f\"Number of vocabulary: {vocabulary_size}\")\n",
    "vectorizer('I love artificial intelligence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2. Bagging\n",
    "\n",
    "I convert each word number into a one-hot encoding and adding all those vectors up."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def get_bag_of_words(text, vocab_size):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text), vocab_size), axis=0)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "ds_train_bow = ds_train.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                       x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size),\n",
    "                                     x['label'])).batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3. Build Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 71s 75ms/step - loss: 0.6155 - acc: 0.8421 - val_loss: 0.4417 - val_acc: 0.8701\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocabulary_size,))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow, validation_data=ds_test_bow)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Bag-of-Words with n-grams\n",
    "\n",
    "Since some words are part of multi-word expressions, for example, the word 'on-line' has a completely different meaning.\n",
    "\n",
    "from the words 'on' and 'line' in other contexts. so the representation of 'on' and 'line' by the same vectors, it can confuse our model.\n",
    "\n",
    "IN n-gram the frequency of each word, bi-word or tri-word is a useful feature for training classifiers, e.g. bigram\n",
    "adds all word pairs to the vocabulary, in addition to original words.\n",
    "\n",
    "#### 2.1 Generate a bi-gram Bag-of-Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Embedding Vectorize as a Layer\n",
    "\n",
    "Since the vectorizer is also a Keras layer, I can define a network that includes it, and train it end-to-end.\n",
    "\n",
    "Then I don't need to vectorize the dataset using map, we can just pass the original dataset to the input of the network.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 117s 124ms/step - loss: 0.5946 - acc: 0.8465 - val_loss: 0.4196 - val_acc: 0.8733\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_23 (Text  (None, None)             0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " tf.one_hot_6 (TFOpLambda)   (None, None, 5335)        0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_6 (TFOpL  (None, 5335)             0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def to_tuple(x):\n",
    "    return x['title'] + ' ' + x['description'], x['label']\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "ds_train_embed = ds_train.map(to_tuple).batch(batch_size)\n",
    "ds_test_embed = ds_test.map(to_tuple).batch(batch_size)\n",
    "\n",
    "inp = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x, vocabulary_size), axis=1)\n",
    "out = keras.layers.Dense(4, activation='softmax')(x)\n",
    "model = keras.models.Model(inp, out)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_embed, validation_data=ds_test_embed)\n",
    "\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
