{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Author: Kaveh Mahdavi <kavehmahdavi74@yahoo.com>\n",
    "License: BSD 3 clause\n",
    "last update: 28/12/2022"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# News Classification\n",
    "\n",
    "I explore different neural network architectures for dealing with natural language text by using:\n",
    "* bag-of-words\n",
    "* embeddings\n",
    "* recurrent neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To use GPU memory cautiously, I set tensorflow option to grow GPU memory allocation when needed.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_bag_of_words(text, vocab_size):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text), vocab_size), axis=0)\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Represent text\n",
    "\n",
    "To solve Natural Language Processing (NLP) tasks with ANN, I need some way to represent text as tensors.\n",
    "\n",
    "* **Character-level representation:** I represent text by treating each character as a number. Given that we have C  different characters in our text corpus, the word Hello could be represented by a tensor with shape C×5. Each letter would correspond to a tensor in one-hot encoding.\n",
    "* **Word-level representation:** I create a vocabulary of all words in our text, and then represent words using one-hot encoding. This approach is better than character-level representation because each letter by itself does not have much meaning. By using higher-level semantic concepts - words - we simplify the task for the neural network. However, given a large dictionary size, we need to deal with high-dimensional sparse tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset 11.24 MiB (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /home/kaveh/tensorflow_datasets/ag_news_subset/1.0.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d620ac804fe74e96a6a5d4c7e8a6ab1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "786967338942417396f00e0e236f4488"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "183aabc3e69c43d89a906693d7d72fc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "214abfc89783459db2c5cda23d6be37a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train examples...:   0%|          | 0/120000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5275256af7c46fba73ebfb72be696b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/kaveh/tensorflow_datasets/ag_news_subset/1.0.0.incompleteYA5A4R/ag_news_subset-train.tfrecord*…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41134e7744e14626bdb31cd32cdf91a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test examples...:   0%|          | 0/7600 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a14acdb38314646a7e86cee7e1806d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/kaveh/tensorflow_datasets/ag_news_subset/1.0.0.incompleteYA5A4R/ag_news_subset-test.tfrecord*.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b57684247f4e4e7c956fab4fabe547ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset ag_news_subset downloaded and prepared to /home/kaveh/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = tfds.load('ag_news_subset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 120000\n",
      "Size of test dataset:  7600\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(\"Size of train dataset: {}\".format(len(ds_train)))\n",
    "print(\"Size of test dataset:  {}\".format(len(ds_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(3), ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Classifier ANN\n",
    "\n",
    "### I. Approach\n",
    "\n",
    "#### Vectorize Text\n",
    "\n",
    "I vectorize text into numbers to represent as tensors. In the word-level, I should do:\n",
    "* Use a tokenizer to split text into tokens.\n",
    "* Build a vocabulary of those tokens.\n",
    "\n",
    "I don't take to account words that are rarely present in the text, since only a few sentences will have them, and the model will not learn from them.\n",
    "I limit the vocabulary size by passing an argument to the `TextVectorization` constructor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Vectorize & build a vocabulary\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=50000)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title'] + ' ' + x['description']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for', '39s', 'with', 'that', 'its', 'as']\n",
      "Number of vocabulary: 5335\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 112, 3695, 5071, 3908])>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_vocabulary()\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary[:15])\n",
    "print(f\"Number of vocabulary: {vocabulary_size}\")\n",
    "vectorizer('I love artificial intelligence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bag of Words\n",
    "\n",
    "I convert each word number into a one-hot encoding and adding all those vectors up."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset element_spec=(TensorSpec(shape=(None, 5335), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "ds_train_bow = ds_train.map(\n",
    "    lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size), x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(\n",
    "    lambda x: (get_bag_of_words(x['title'] + x['description'], vocabulary_size), x['label'])).batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build a Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 60s 63ms/step - loss: 0.6170 - acc: 0.8416 - val_loss: 0.4418 - val_acc: 0.8707\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f4813659af0>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocabulary_size,))\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow, validation_data=ds_test_bow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II. Approach:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "429/938 [============>.................] - ETA: 4s - loss: 0.7596 - acc: 0.8135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7f4839707310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/weakref.py\", line 345, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[60], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39madapt(ds_train\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m500\u001B[39m)\u001B[38;5;241m.\u001B[39mmap(extract_text))\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m,metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 8\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtupelize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mds_test\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtupelize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/engine/training.py:1656\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1654\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[1;32m   1655\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[0;32m-> 1656\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[1;32m   1658\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:476\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \n\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[0;32m--> 476\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:323\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[0;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[1;32m    321\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 323\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    328\u001B[0m     )\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:346\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[0;34m(self, mode, batch, logs)\u001B[0m\n\u001B[1;32m    343\u001B[0m     batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[0;32m--> 346\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[1;32m    349\u001B[0m     end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:394\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[0;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m    393\u001B[0m     hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[0;32m--> 394\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:1094\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m-> 1094\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:1156\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1154\u001B[0m \u001B[38;5;124;03m\"\"\"Updates the progbar.\"\"\"\u001B[39;00m\n\u001B[1;32m   1155\u001B[0m logs \u001B[38;5;241m=\u001B[39m logs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m-> 1156\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_init_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_steps:\n\u001B[1;32m   1158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m=\u001B[39m batch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# One-indexed.\u001B[39;00m\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/callbacks.py:1131\u001B[0m, in \u001B[0;36mProgbarLogger._maybe_init_progbar\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstateful_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstateful_metrics)\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m   1127\u001B[0m     \u001B[38;5;66;03m# Update the existing stateful metrics as `self.model.metrics` may\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m     \u001B[38;5;66;03m# contain updated metrics after `MetricsContainer` is built in the\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;66;03m# first train step.\u001B[39;00m\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstateful_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstateful_metrics\u001B[38;5;241m.\u001B[39munion(\n\u001B[0;32m-> 1131\u001B[0m         \u001B[38;5;28mset\u001B[39m(m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetrics\u001B[49m)\n\u001B[1;32m   1132\u001B[0m     )\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar \u001B[38;5;241m=\u001B[39m Progbar(\n\u001B[1;32m   1136\u001B[0m         target\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget,\n\u001B[1;32m   1137\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m   1138\u001B[0m         stateful_metrics\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstateful_metrics,\n\u001B[1;32m   1139\u001B[0m         unit_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_steps \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1140\u001B[0m     )\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/engine/training.py:848\u001B[0m, in \u001B[0;36mModel.metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    844\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_compiled:\n\u001B[1;32m    845\u001B[0m     \u001B[38;5;66;03m# TODO(omalleyt): Track `LossesContainer` and `MetricsContainer`\u001B[39;00m\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;66;03m# objects so that attr names are not load-bearing.\u001B[39;00m\n\u001B[1;32m    847\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 848\u001B[0m         metrics \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetrics\u001B[49m\n\u001B[1;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    850\u001B[0m         metrics \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_metrics\u001B[38;5;241m.\u001B[39mmetrics\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/keras/engine/compile_utils.py:171\u001B[0m, in \u001B[0;36mLossesContainer.metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_built:\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[1;32m    169\u001B[0m per_output_metrics \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    170\u001B[0m     metric_obj\n\u001B[0;32m--> 171\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m metric_obj \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_per_output_metrics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m metric_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    173\u001B[0m ]\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_total_loss_mean] \u001B[38;5;241m+\u001B[39m per_output_metrics\n",
      "File \u001B[0;32m~/software/pycharm-community-2016.3.2/PycharmProjects/venv/TF_Portfolio/lib/python3.8/site-packages/tensorflow/python/util/nest.py:454\u001B[0m, in \u001B[0;36mflatten\u001B[0;34m(structure, expand_composites)\u001B[0m\n\u001B[1;32m    452\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m    453\u001B[0m expand_composites \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbool\u001B[39m(expand_composites)\n\u001B[0;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_pywrap_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFlatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstructure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpand_composites\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(1,),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x,vocabulary_size),axis=1)\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.5911 - acc: 0.8505 - val_loss: 0.4166 - val_acc: 0.8776\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocabulary_size,output_mode='count'),\n",
    "    keras.layers.Dense(4,input_shape=(vocabulary_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
